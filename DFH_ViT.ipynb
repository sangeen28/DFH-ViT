{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "30f7ef2f8eeb484eaaecff250a931d3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98f7474d7aa54b94b454c100412e3458",
              "IPY_MODEL_7af95713bc95422f84f983dd5dc1281e",
              "IPY_MODEL_f129d6a2b57f4dcaa392f45d6cdad5b7"
            ],
            "layout": "IPY_MODEL_e04d99d66e564dff85f66b26c1b5a22c"
          }
        },
        "98f7474d7aa54b94b454c100412e3458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01b2b540048f41558cf6003da766f17d",
            "placeholder": "​",
            "style": "IPY_MODEL_29229a9552454b349da0b05611cbae4c",
            "value": "model.safetensors: 100%"
          }
        },
        "7af95713bc95422f84f983dd5dc1281e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aef7e7e803e48e1abd79d9c0cd575c1",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ca4e9cbd8084127aa7615ee6435c50f",
            "value": 346284714
          }
        },
        "f129d6a2b57f4dcaa392f45d6cdad5b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b41928d3d65146e4b81f5539a0ccca8f",
            "placeholder": "​",
            "style": "IPY_MODEL_a33ae500a3804e59ba1df49d406bb9e8",
            "value": " 346M/346M [00:02&lt;00:00, 305MB/s]"
          }
        },
        "e04d99d66e564dff85f66b26c1b5a22c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01b2b540048f41558cf6003da766f17d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29229a9552454b349da0b05611cbae4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5aef7e7e803e48e1abd79d9c0cd575c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ca4e9cbd8084127aa7615ee6435c50f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b41928d3d65146e4b81f5539a0ccca8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a33ae500a3804e59ba1df49d406bb9e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/data\n",
        "%cd /content/data\n",
        "\n",
        "# Endless Forams combined training set\n",
        "!wget -O Endless_Forams_training_set.zip \\\n",
        "  \"https://zenodo.org/records/3996436/files/Endless_Forams_training_set.zip?download=1\"\n",
        "!unzip -q Endless_Forams_training_set.zip\n",
        "\n",
        "# Core MD022508 training set\n",
        "!wget -O MD022508_training_set.zip \\\n",
        "  \"https://zenodo.org/records/3996436/files/MD022508_training_set.zip?download=1\"\n",
        "!unzip -q MD022508_training_set.zip\n",
        "\n",
        "# Core MD972138 training set\n",
        "!wget -O MD972138_training_set.zip \\\n",
        "  \"https://zenodo.org/records/3996436/files/MD972138_training_set.zip?download=1\"\n",
        "!unzip -q MD972138_training_set.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpJvRXhG2YFr",
        "outputId": "41d48cd7-25a5-4250-b09e-dace8432b31c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n",
            "--2025-12-15 06:01:07--  https://zenodo.org/records/3996436/files/Endless_Forams_training_set.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.153, 188.185.48.75, 137.138.52.235, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 714725290 (682M) [application/octet-stream]\n",
            "Saving to: ‘Endless_Forams_training_set.zip’\n",
            "\n",
            "Endless_Forams_trai 100%[===================>] 681.61M  5.54MB/s    in 12m 29s \n",
            "\n",
            "2025-12-15 06:13:37 (932 KB/s) - ‘Endless_Forams_training_set.zip’ saved [714725290/714725290]\n",
            "\n",
            "--2025-12-15 06:13:47--  https://zenodo.org/records/3996436/files/MD022508_training_set.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 137.138.52.235, 188.185.43.153, 188.185.48.75, ...\n",
            "Connecting to zenodo.org (zenodo.org)|137.138.52.235|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 824795913 (787M) [application/octet-stream]\n",
            "Saving to: ‘MD022508_training_set.zip’\n",
            "\n",
            "MD022508_training_s 100%[===================>] 786.59M  12.9MB/s    in 4m 26s  \n",
            "\n",
            "2025-12-15 06:18:14 (2.96 MB/s) - ‘MD022508_training_set.zip’ saved [824795913/824795913]\n",
            "\n",
            "--2025-12-15 06:18:23--  https://zenodo.org/records/3996436/files/MD972138_training_set.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.153, 137.138.52.235, 188.185.48.75, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 676336889 (645M) [application/octet-stream]\n",
            "Saving to: ‘MD972138_training_set.zip’\n",
            "\n",
            "MD972138_training_s 100%[===================>] 645.00M  12.6MB/s    in 7m 17s  \n",
            "\n",
            "2025-12-15 06:25:40 (1.48 MB/s) - ‘MD972138_training_set.zip’ saved [676336889/676336889]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\n",
        "!pip install -q timm torchmetrics albumentations\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from typing import Optional, Dict, Any, List\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "import timm\n",
        "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score\n",
        "\n",
        "\n",
        "class CFG:\n",
        "    # labels (will be overridden after scanning Endless Forams)\n",
        "    NUM_FINE   = 35   # placeholder; updated dynamically\n",
        "    NUM_COARSE = 2    # simple hierarchy: first half vs second half\n",
        "\n",
        "    # training\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_WORKERS = 2   # set 0 if debugging DataLoader errors\n",
        "    IMAGE_SIZE = 224\n",
        "    EPOCHS_PRETRAIN = 2        # small for demo\n",
        "    EPOCHS_FINETUNE = 2\n",
        "    LR_PRETRAIN = 3e-4\n",
        "    LR_FINETUNE = 1e-4\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # losses / weights\n",
        "    LAMBDA_FINE_CE = 1.0\n",
        "    LAMBDA_METRIC  = 0.1\n",
        "    TEMP_CONTRAST  = 0.1\n",
        "\n",
        "    # pretraining multi-task weights\n",
        "    ALPHA_SUP = 1.0\n",
        "    BETA_MAE  = 0.5\n",
        "    GAMMA_AUX = 0.2\n",
        "\n",
        "    # active learning\n",
        "    MC_DROPOUT_SAMPLES = 8\n",
        "    ACTIVE_TOP_K = 200\n",
        "\n",
        "cfg = CFG()\n",
        "print(\"Using device:\", cfg.DEVICE)\n",
        "\n",
        "\n",
        "def supervised_contrastive_loss(embeddings: torch.Tensor,\n",
        "                                labels: torch.Tensor,\n",
        "                                temperature: float = 0.1):\n",
        "    \"\"\"\n",
        "    embeddings: (B, D)\n",
        "    labels:     (B,)\n",
        "    \"\"\"\n",
        "    device = embeddings.device\n",
        "    z = F.normalize(embeddings, dim=-1)\n",
        "    logits = torch.matmul(z, z.T) / temperature  # (B, B)\n",
        "    labels = labels.view(-1, 1)\n",
        "    mask = torch.eq(labels, labels.T).float().to(device)\n",
        "\n",
        "    # remove self-comparisons\n",
        "    logits_mask = torch.ones_like(mask) - torch.eye(mask.size(0), device=device)\n",
        "    mask = mask * logits_mask\n",
        "\n",
        "    # log-softmax over rows\n",
        "    logits_max, _ = torch.max(logits, dim=1, keepdim=True)\n",
        "    logits = logits - logits_max.detach()\n",
        "    exp_logits = torch.exp(logits) * logits_mask\n",
        "    log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-12)\n",
        "\n",
        "    pos_count = mask.sum(1)\n",
        "    pos_count = torch.clamp(pos_count, min=1.0)\n",
        "    mean_log_prob_pos = (mask * log_prob).sum(1) / pos_count\n",
        "\n",
        "    loss = -mean_log_prob_pos.mean()\n",
        "    return loss\n",
        "\n",
        "\n",
        "class DFHViT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 backbone_name: str = \"vit_base_patch16_224\",\n",
        "                 num_fine: int = 10,\n",
        "                 num_coarse: int = 2,\n",
        "                 metadata_dim: Optional[int] = None):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model(\n",
        "            backbone_name,\n",
        "            pretrained=True,\n",
        "            num_classes=0,     # feature extractor only\n",
        "            global_pool=\"avg\",\n",
        "        )\n",
        "        self.embed_dim = self.backbone.num_features\n",
        "\n",
        "        self.use_metadata = metadata_dim is not None\n",
        "        if self.use_metadata:\n",
        "            self.meta_mlp = nn.Sequential(\n",
        "                nn.Linear(metadata_dim, self.embed_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(self.embed_dim, self.embed_dim)\n",
        "            )\n",
        "\n",
        "        # hierarchical heads\n",
        "        self.coarse_head = nn.Linear(self.embed_dim, num_coarse)\n",
        "        self.fine_head = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim + num_coarse, self.embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.embed_dim, num_fine)\n",
        "        )\n",
        "\n",
        "        # reconstruction head (32x32 RGB)\n",
        "        self.recon_head = nn.Sequential(\n",
        "            nn.Linear(self.embed_dim, 3 * 32 * 32),\n",
        "        )\n",
        "        # aux head (e.g., rotation prediction, 4 classes)\n",
        "        self.aux_head = nn.Linear(self.embed_dim, 4)\n",
        "\n",
        "    def encode(self, images: torch.Tensor, metadata: Optional[torch.Tensor] = None):\n",
        "        z = self.backbone(images)   # (B, D)\n",
        "        if self.use_metadata and metadata is not None:\n",
        "            m = self.meta_mlp(metadata)\n",
        "            z = z + m\n",
        "        return z\n",
        "\n",
        "    def forward(self, images: torch.Tensor, metadata: Optional[torch.Tensor] = None):\n",
        "        z = self.encode(images, metadata)\n",
        "        coarse_logits = self.coarse_head(z)\n",
        "        fine_input = torch.cat([z, coarse_logits], dim=-1)\n",
        "        fine_logits = self.fine_head(fine_input)\n",
        "\n",
        "        recon_logits = self.recon_head(z)\n",
        "        aux_logits   = self.aux_head(z)\n",
        "        return coarse_logits, fine_logits, z, recon_logits, aux_logits\n",
        "\n",
        "\n",
        "class SyntheticFossilFractalDataset(Dataset):\n",
        "    def __init__(self, num_samples: int = 20000, image_size: int = 224, transform=None):\n",
        "        self.num_samples = num_samples\n",
        "        self.image_size = image_size\n",
        "        self.transform = transform\n",
        "\n",
        "        self.num_coarse = cfg.NUM_COARSE\n",
        "        self.num_fine   = cfg.NUM_FINE\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def _draw_spumellarian_like(self, draw: ImageDraw.Draw, size: int):\n",
        "        cx, cy = size // 2, size // 2\n",
        "        r = random.randint(size//6, size//3)\n",
        "        draw.ellipse([cx-r, cy-r, cx+r, cy+r], outline=\"white\", width=2)\n",
        "        for _ in range(random.randint(6, 20)):\n",
        "            angle = random.uniform(0, 2*math.pi)\n",
        "            r2 = random.randint(r, size//2)\n",
        "            x2 = cx + int(r2 * math.cos(angle))\n",
        "            y2 = cy + int(r2 * math.sin(angle))\n",
        "            draw.line([cx, cy, x2, y2], fill=\"white\", width=1)\n",
        "\n",
        "    def _draw_nassellarian_like(self, draw: ImageDraw.Draw, size: int):\n",
        "        cx, cy = size // 2, size // 2\n",
        "        top = cy - size//4\n",
        "        bottom = cy + size//4\n",
        "        w = size//6\n",
        "        draw.polygon([(cx, top), (cx-w, bottom), (cx+w, bottom)],\n",
        "                     outline=\"white\", width=2)\n",
        "        for i in range(4):\n",
        "            y = top + (bottom - top) * (i+1) / 5\n",
        "            draw.line([cx-w+2, y, cx+w-2, y], fill=\"white\", width=1)\n",
        "\n",
        "    def _draw_diatom_like(self, draw: ImageDraw.Draw, size: int):\n",
        "        cx, cy = size // 2, size // 2\n",
        "        w = size//4\n",
        "        h = size//8\n",
        "        draw.ellipse([cx-w, cy-h, cx+w, cy+h], outline=\"white\", width=2)\n",
        "        for _ in range(50):\n",
        "            x = random.randint(cx-w, cx+w)\n",
        "            y = random.randint(cy-h, cy+h)\n",
        "            draw.point((x, y), fill=\"white\")\n",
        "\n",
        "    def _generate_image_and_labels(self):\n",
        "        img = Image.new(\"L\", (self.image_size, self.image_size), color=0)\n",
        "        draw = ImageDraw.Draw(img)\n",
        "\n",
        "        coarse = random.randint(0, self.num_coarse - 1)\n",
        "        if coarse == 0:\n",
        "            self._draw_spumellarian_like(draw, self.image_size)\n",
        "        elif coarse == 1:\n",
        "            self._draw_nassellarian_like(draw, self.image_size)\n",
        "        else:\n",
        "            self._draw_diatom_like(draw, self.image_size)\n",
        "\n",
        "        fine = random.randint(0, self.num_fine - 1)\n",
        "        img = img.convert(\"RGB\")\n",
        "        return img, coarse, fine\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, coarse, fine = self._generate_image_and_labels()\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return {\n",
        "            \"image\": img,\n",
        "            \"coarse_label\": torch.tensor(coarse, dtype=torch.long),\n",
        "            \"fine_label\":   torch.tensor(fine,   dtype=torch.long),\n",
        "        }\n",
        "\n",
        "\n",
        "class EndlessForamsHierDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Hierarchical dataset for Endless Forams:\n",
        "    - fine_label: species index (0..num_species-1)\n",
        "    - coarse_label: simple binary split over species indices\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        samples,\n",
        "        class_to_idx,\n",
        "        transform=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        samples: list of (image_path, fine_label)\n",
        "        class_to_idx: dict {species_name: index}\n",
        "        \"\"\"\n",
        "        self.samples = samples\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.transform = transform\n",
        "\n",
        "        self.classes = sorted(class_to_idx, key=lambda k: class_to_idx[k])\n",
        "        self.num_classes = len(self.classes)\n",
        "\n",
        "        # simple 2-way hierarchy: first half vs second half of classes\n",
        "        midpoint = self.num_classes // 2\n",
        "        self.coarse_map = {\n",
        "            i: 0 if i < midpoint else 1\n",
        "            for i in range(self.num_classes)\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def build_from_root(\n",
        "        cls,\n",
        "        root_dir: str,\n",
        "        transform_train=None,\n",
        "        transform_val=None,\n",
        "        val_ratio: float = 0.2,\n",
        "        seed: int = 42,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Scans root_dir/<species_name>/*.jpg and creates\n",
        "        train/val splits and a shared class_to_idx mapping.\n",
        "        \"\"\"\n",
        "        # discover species (subfolders)\n",
        "        classes = sorted(\n",
        "            d for d in os.listdir(root_dir)\n",
        "            if os.path.isdir(os.path.join(root_dir, d))\n",
        "        )\n",
        "        class_to_idx = {name: i for i, name in enumerate(classes)}\n",
        "\n",
        "        # collect (path, fine_label)\n",
        "        all_samples = []\n",
        "        for name in classes:\n",
        "            class_dir = os.path.join(root_dir, name)\n",
        "            for fname in os.listdir(class_dir):\n",
        "                if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "                    full_path = os.path.join(class_dir, fname)\n",
        "                    all_samples.append((full_path, class_to_idx[name]))\n",
        "\n",
        "        indices = np.arange(len(all_samples))\n",
        "        rng = np.random.RandomState(seed)\n",
        "        rng.shuffle(indices)\n",
        "\n",
        "        split = int((1.0 - val_ratio) * len(indices))\n",
        "        train_idx = indices[:split]\n",
        "        val_idx   = indices[split:]\n",
        "\n",
        "        train_samples = [all_samples[i] for i in train_idx]\n",
        "        val_samples   = [all_samples[i] for i in val_idx]\n",
        "\n",
        "        train_ds = cls(train_samples, class_to_idx, transform_train)\n",
        "        val_ds   = cls(val_samples,   class_to_idx, transform_val)\n",
        "        return train_ds, val_ds, class_to_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, fine = self.samples[idx]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        coarse = self.coarse_map[fine]\n",
        "        return {\n",
        "            \"image\": img,\n",
        "            \"coarse_label\": torch.tensor(coarse, dtype=torch.long),\n",
        "            \"fine_label\":   torch.tensor(fine,   dtype=torch.long),\n",
        "        }\n",
        "\n",
        "\n",
        "class EndlessForamsUnlabeledPool(Dataset):\n",
        "    \"\"\"\n",
        "    Unlabeled pool built from MD022508 and MD972138 training sets.\n",
        "    Returns only image + index (no labels), for active learning / pseudo-labeling.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dirs, transform=None):\n",
        "        \"\"\"\n",
        "        root_dirs: list of roots like\n",
        "           [\"/content/data/MD022508_training_set/MD022508_training_set\",\n",
        "            \"/content/data/MD972138_training_set/MD972138_training_set\"]\n",
        "        \"\"\"\n",
        "        self.transform = transform\n",
        "        self.paths = []\n",
        "\n",
        "        for root_dir in root_dirs:\n",
        "            if not os.path.isdir(root_dir):\n",
        "                continue\n",
        "            species_dirs = [\n",
        "                d for d in os.listdir(root_dir)\n",
        "                if os.path.isdir(os.path.join(root_dir, d))\n",
        "            ]\n",
        "            for s in species_dirs:\n",
        "                class_dir = os.path.join(root_dir, s)\n",
        "                for fname in os.listdir(class_dir):\n",
        "                    if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "                        self.paths.append(os.path.join(class_dir, fname))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.paths[idx]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return {\n",
        "            \"image\": img,\n",
        "            \"index\": idx,\n",
        "        }\n",
        "\n",
        "\n",
        "# ===================== Transforms ============================\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((cfg.IMAGE_SIZE, cfg.IMAGE_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(cfg.IMAGE_SIZE, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((cfg.IMAGE_SIZE, cfg.IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "# ===================== Dataset paths =========================\n",
        "ENDLESS_ROOT   = \"/content/data/Endless_Forams_training_set\"\n",
        "MD022508_ROOT  = \"/content/data/MD022508_training_set\"\n",
        "MD972138_ROOT  = \"/content/data/MD972138_training_set\"\n",
        "\n",
        "assert os.path.isdir(ENDLESS_ROOT), f\"Endless Forams root not found: {ENDLESS_ROOT}\"\n",
        "\n",
        "\n",
        "# Build train/val splits for Endless Forams training set\n",
        "train_real_dataset, val_real_dataset, class_to_idx = EndlessForamsHierDataset.build_from_root(\n",
        "    root_dir=ENDLESS_ROOT,\n",
        "    transform_train=train_transform,\n",
        "    transform_val=val_transform,\n",
        "    val_ratio=0.2,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "# Update config based on discovered classes\n",
        "cfg.NUM_FINE   = len(class_to_idx)\n",
        "cfg.NUM_COARSE = 2  # we defined a 2-way hierarchy above\n",
        "\n",
        "print(f\"NUM_FINE (species classes) = {cfg.NUM_FINE}\")\n",
        "print(f\"NUM_COARSE (coarse groups) = {cfg.NUM_COARSE}\")\n",
        "\n",
        "train_real_loader = DataLoader(\n",
        "    train_real_dataset,\n",
        "    batch_size=cfg.BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=cfg.NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "val_real_loader = DataLoader(\n",
        "    val_real_dataset,\n",
        "    batch_size=cfg.BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=cfg.NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "# Synthetic pretraining data (after cfg.NUM_FINE is updated)\n",
        "synthetic_dataset = SyntheticFossilFractalDataset(\n",
        "    num_samples=10000,\n",
        "    image_size=cfg.IMAGE_SIZE,\n",
        "    transform=train_transform,\n",
        ")\n",
        "synthetic_loader = DataLoader(\n",
        "    synthetic_dataset,\n",
        "    batch_size=cfg.BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=cfg.NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "# Unlabeled pool for active learning demo\n",
        "unlabeled_pool_dataset = EndlessForamsUnlabeledPool(\n",
        "    root_dirs=[MD022508_ROOT, MD972138_ROOT],\n",
        "    transform=val_transform,\n",
        ")\n",
        "\n",
        "print(f\"Train images (Endless Forams): {len(train_real_dataset)}\")\n",
        "print(f\"Val images   (Endless Forams): {len(val_real_dataset)}\")\n",
        "print(f\"Unlabeled pool images (MD022508 + MD972138): {len(unlabeled_pool_dataset)}\")\n",
        "\n",
        "\n",
        "def pretrain_epoch(model: DFHViT,\n",
        "                   dataloader: DataLoader,\n",
        "                   optimizer: torch.optim.Optimizer,\n",
        "                   device: str = \"cuda\"):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in dataloader:\n",
        "        images = batch[\"image\"].to(device)\n",
        "        coarse_labels = batch[\"coarse_label\"].to(device)\n",
        "        fine_labels   = batch[\"fine_label\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        coarse_logits, fine_logits, z, recon_logits, aux_logits = model(images, metadata=None)\n",
        "\n",
        "        loss_coarse = F.cross_entropy(coarse_logits, coarse_labels)\n",
        "        loss_fine   = F.cross_entropy(fine_logits, fine_labels)\n",
        "        L_sup = loss_coarse + loss_fine\n",
        "\n",
        "        with torch.no_grad():\n",
        "            img_small = F.interpolate(images, size=(32, 32),\n",
        "                                      mode=\"bilinear\", align_corners=False)\n",
        "        img_target = img_small.view(images.size(0), -1)\n",
        "        L_mae = F.mse_loss(recon_logits, img_target)\n",
        "\n",
        "        # dummy aux loss: predict rotation class using fine_labels % 4\n",
        "        L_aux = F.cross_entropy(aux_logits, fine_labels % 4)\n",
        "\n",
        "        L_pre = cfg.ALPHA_SUP * L_sup + cfg.BETA_MAE * L_mae + cfg.GAMMA_AUX * L_aux\n",
        "        L_pre.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += L_pre.item() * images.size(0)\n",
        "\n",
        "    return total_loss / len(dataloader.dataset)\n",
        "\n",
        "\n",
        "def finetune_epoch(model: DFHViT,\n",
        "                   dataloader: DataLoader,\n",
        "                   optimizer: torch.optim.Optimizer,\n",
        "                   device: str = \"cuda\"):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in dataloader:\n",
        "        images = batch[\"image\"].to(device)\n",
        "        coarse_labels = batch[\"coarse_label\"].to(device)\n",
        "        fine_labels   = batch[\"fine_label\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        coarse_logits, fine_logits, z, recon_logits, aux_logits = model(images, metadata=None)\n",
        "\n",
        "        loss_coarse = F.cross_entropy(coarse_logits, coarse_labels)\n",
        "        loss_fine   = F.cross_entropy(fine_logits, fine_labels)\n",
        "        L_ce = loss_coarse + cfg.LAMBDA_FINE_CE * loss_fine\n",
        "\n",
        "        L_metric = supervised_contrastive_loss(z, fine_labels, temperature=cfg.TEMP_CONTRAST)\n",
        "        loss = L_ce + cfg.LAMBDA_METRIC * L_metric\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "\n",
        "    return total_loss / len(dataloader.dataset)\n",
        "\n",
        "\n",
        "def evaluate(model: DFHViT,\n",
        "             dataloader: DataLoader,\n",
        "             device: str = \"cuda\"):\n",
        "    model.eval()\n",
        "    coarse_acc = MulticlassAccuracy(num_classes=cfg.NUM_COARSE).to(device)\n",
        "    fine_acc   = MulticlassAccuracy(num_classes=cfg.NUM_FINE).to(device)\n",
        "    fine_f1    = MulticlassF1Score(num_classes=cfg.NUM_FINE, average=\"macro\").to(device)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            images = batch[\"image\"].to(device)\n",
        "            coarse_labels = batch[\"coarse_label\"].to(device)\n",
        "            fine_labels   = batch[\"fine_label\"].to(device)\n",
        "\n",
        "            coarse_logits, fine_logits, z, recon_logits, aux_logits = model(images, metadata=None)\n",
        "\n",
        "            loss_coarse = F.cross_entropy(coarse_logits, coarse_labels)\n",
        "            loss_fine   = F.cross_entropy(fine_logits, fine_labels)\n",
        "            L_ce = loss_coarse + cfg.LAMBDA_FINE_CE * loss_fine\n",
        "            total_loss += L_ce.item() * images.size(0)\n",
        "\n",
        "            coarse_acc.update(coarse_logits, coarse_labels)\n",
        "            fine_acc.update(fine_logits, fine_labels)\n",
        "            fine_f1.update(fine_logits, fine_labels)\n",
        "\n",
        "    n = len(dataloader.dataset)\n",
        "    metrics = {\n",
        "        \"loss\": total_loss / n,\n",
        "        \"coarse_acc\": coarse_acc.compute().item(),\n",
        "        \"fine_acc\":   fine_acc.compute().item(),\n",
        "        \"fine_f1\":    fine_f1.compute().item(),\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def predictive_entropy_mc_dropout(model: DFHViT,\n",
        "                                  images: torch.Tensor,\n",
        "                                  metadata: Optional[torch.Tensor],\n",
        "                                  mc_samples: int = 8,\n",
        "                                  device: str = \"cuda\"):\n",
        "    model.train()  # keep dropout active\n",
        "    images = images.to(device)\n",
        "\n",
        "    probs_list = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(mc_samples):\n",
        "            _, fine_logits, _, _, _ = model(images, metadata)\n",
        "            probs = F.softmax(fine_logits, dim=-1)\n",
        "            probs_list.append(probs)\n",
        "\n",
        "    probs_stack = torch.stack(probs_list, dim=0)   # (T, B, K)\n",
        "    probs_mean = probs_stack.mean(dim=0)           # (B, K)\n",
        "    entropy = -(probs_mean * (probs_mean.clamp_min(1e-12).log())).sum(dim=-1)\n",
        "    return entropy.cpu()\n",
        "\n",
        "\n",
        "def select_uncertain_samples(model: DFHViT,\n",
        "                             pool_dataset: Dataset,\n",
        "                             batch_size: int = 128,\n",
        "                             top_k: int = 200,\n",
        "                             device: str = \"cuda\"):\n",
        "    model.to(device)\n",
        "    model.eval()  # we'll flip to train inside entropy fn\n",
        "\n",
        "    pool_loader = DataLoader(pool_dataset, batch_size=batch_size,\n",
        "                             shuffle=False, num_workers=cfg.NUM_WORKERS)\n",
        "\n",
        "    all_indices = []\n",
        "    all_entropy = []\n",
        "    for batch in pool_loader:\n",
        "        images = batch[\"image\"]\n",
        "        idxs = batch[\"index\"]\n",
        "        entropy = predictive_entropy_mc_dropout(\n",
        "            model, images, metadata=None,\n",
        "            mc_samples=cfg.MC_DROPOUT_SAMPLES,\n",
        "            device=device,\n",
        "        )\n",
        "        all_indices.extend(idxs.numpy().tolist())\n",
        "        all_entropy.extend(entropy.numpy().tolist())\n",
        "\n",
        "    all_indices = np.array(all_indices)\n",
        "    all_entropy = np.array(all_entropy)\n",
        "\n",
        "    top_k = min(top_k, len(all_indices))\n",
        "    selected_idx = np.argpartition(-all_entropy, top_k-1)[:top_k]\n",
        "    pool_indices_selected = all_indices[selected_idx]\n",
        "    ent_selected = all_entropy[selected_idx]\n",
        "    return pool_indices_selected, ent_selected\n",
        "\n",
        "\n",
        "def run_training():\n",
        "    device = cfg.DEVICE\n",
        "    model = DFHViT(\n",
        "        backbone_name=\"vit_base_patch16_224\",\n",
        "        num_fine=cfg.NUM_FINE,\n",
        "        num_coarse=cfg.NUM_COARSE,\n",
        "        metadata_dim=None,\n",
        "    ).to(device)\n",
        "\n",
        "    print(\"----- Pretraining on synthetic fractal data -----\")\n",
        "    optimizer_pre = torch.optim.Adam(model.parameters(), lr=cfg.LR_PRETRAIN)\n",
        "    for epoch in range(cfg.EPOCHS_PRETRAIN):\n",
        "        loss_pre = pretrain_epoch(model, synthetic_loader, optimizer_pre, device=device)\n",
        "        print(f\"[Pretrain] Epoch {epoch+1}/{cfg.EPOCHS_PRETRAIN} - loss: {loss_pre:.4f}\")\n",
        "\n",
        "    print(\"\\n----- Fine-tuning on Endless Forams -----\")\n",
        "    optimizer_ft = torch.optim.Adam(model.parameters(), lr=cfg.LR_FINETUNE)\n",
        "    best_f1 = 0.0\n",
        "    for epoch in range(cfg.EPOCHS_FINETUNE):\n",
        "        train_loss = finetune_epoch(model, train_real_loader, optimizer_ft, device=device)\n",
        "        val_metrics = evaluate(model, val_real_loader, device=device)\n",
        "        print(f\"[Finetune] Epoch {epoch+1}/{cfg.EPOCHS_FINETUNE} \"\n",
        "              f\"train_loss={train_loss:.4f} \"\n",
        "              f\"val_loss={val_metrics['loss']:.4f} \"\n",
        "              f\"coarse_acc={val_metrics['coarse_acc']:.4f} \"\n",
        "              f\"fine_acc={val_metrics['fine_acc']:.4f} \"\n",
        "              f\"fine_f1={val_metrics['fine_f1']:.4f}\")\n",
        "\n",
        "        if val_metrics[\"fine_f1\"] > best_f1:\n",
        "            best_f1 = val_metrics[\"fine_f1\"]\n",
        "            torch.save(model.state_dict(), \"/content/dfh_vit_forams_best.pth\")\n",
        "            print(\"  -> saved best model (F1 = {:.4f})\".format(best_f1))\n",
        "    return model\n",
        "\n",
        "\n",
        "# ------------------- Run training ----------------------------\n",
        "model = run_training()\n",
        "\n",
        "# After training, you can run one active learning round:\n",
        "selected_indices, entropies = select_uncertain_samples(\n",
        "     model, unlabeled_pool_dataset,\n",
        "     top_k=cfg.ACTIVE_TOP_K,\n",
        "     device=cfg.DEVICE,\n",
        ")\n",
        "print(\"Selected pool indices (most uncertain):\", selected_indices[:20])\n",
        "\n",
        "\n",
        "# === One-shot evaluation & visualization cell ===\n",
        "# This assumes CFG, DFHViT, val_real_loader, train_real_dataset, and evaluate()\n",
        "# are already defined and that you saved the best model to:\n",
        "# \"/content/dfh_vit_forams_best.pth\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Install sklearn if not available (for confusion_matrix)\n",
        "try:\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "except ImportError:\n",
        "    !pip install -q scikit-learn\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "\n",
        "device = cfg.DEVICE\n",
        "\n",
        "# 1) Load best model\n",
        "model = DFHViT(\n",
        "    backbone_name=\"vit_base_patch16_224\",\n",
        "    num_fine=cfg.NUM_FINE,\n",
        "    num_coarse=cfg.NUM_COARSE,\n",
        "    metadata_dim=None,\n",
        ").to(device)\n",
        "\n",
        "state_dict = torch.load(\"/content/dfh_vit_forams_best.pth\", map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\n",
        "# 2) Overall metrics on validation set\n",
        "val_metrics = evaluate(model, val_real_loader, device=device)\n",
        "print(\"=== Overall Validation Metrics ===\")\n",
        "for k, v in val_metrics.items():\n",
        "    print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "# 3) Collect predictions and labels for confusion matrix & per-class accuracy\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_real_loader:\n",
        "        images = batch[\"image\"].to(device)\n",
        "        labels = batch[\"fine_label\"].to(device)\n",
        "        _, fine_logits, _, _, _ = model(images, metadata=None)\n",
        "        preds = fine_logits.argmax(dim=1)\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "all_preds = np.concatenate(all_preds)\n",
        "all_labels = np.concatenate(all_labels)\n",
        "\n",
        "# 4) Confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds, labels=list(range(cfg.NUM_FINE)))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(cm, interpolation=\"nearest\")\n",
        "plt.title(\"Confusion Matrix (Fine Labels)\")\n",
        "plt.colorbar()\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5) Per-class accuracy bar plot\n",
        "per_class_acc = cm.diagonal() / cm.sum(axis=1).clip(min=1)\n",
        "\n",
        "# Get class names from EndlessForamsHierDataset\n",
        "if hasattr(train_real_dataset, \"classes\"):\n",
        "    class_names = train_real_dataset.classes\n",
        "else:\n",
        "    class_names = [f\"cls_{i}\" for i in range(cfg.NUM_FINE)]\n",
        "\n",
        "plt.figure(figsize=(max(10, cfg.NUM_FINE * 0.4), 4))\n",
        "plt.bar(range(cfg.NUM_FINE), per_class_acc)\n",
        "plt.xticks(range(cfg.NUM_FINE), class_names, rotation=90, ha=\"right\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Per-Class Accuracy (Fine Labels)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6) Show a grid of example predictions\n",
        "def show_example_predictions(model, dataloader, n=16):\n",
        "    model.eval()\n",
        "    images_shown = 0\n",
        "    rows = 4\n",
        "    cols = 4\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(8, 8))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            images = batch[\"image\"].to(device)\n",
        "            labels = batch[\"fine_label\"].to(device)\n",
        "            _, fine_logits, _, _, _ = model(images, metadata=None)\n",
        "            preds = fine_logits.argmax(dim=1)\n",
        "\n",
        "            for i in range(images.size(0)):\n",
        "                if images_shown >= n:\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "                    return\n",
        "                ax = axes[images_shown]\n",
        "                img = images[i].cpu().permute(1, 2, 0).numpy()\n",
        "                true_label = labels[i].item()\n",
        "                pred_label = preds[i].item()\n",
        "                ax.imshow(img)\n",
        "                ax.axis(\"off\")\n",
        "                t_name = class_names[true_label] if true_label < len(class_names) else str(true_label)\n",
        "                p_name = class_names[pred_label] if pred_label < len(class_names) else str(pred_label)\n",
        "                ax.set_title(f\"T:{t_name}\\nP:{p_name}\", fontsize=8)\n",
        "                images_shown += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n=== Example Predictions (first 16 val images) ===\")\n",
        "show_example_predictions(model, val_real_loader, n=16)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468,
          "referenced_widgets": [
            "30f7ef2f8eeb484eaaecff250a931d3e",
            "98f7474d7aa54b94b454c100412e3458",
            "7af95713bc95422f84f983dd5dc1281e",
            "f129d6a2b57f4dcaa392f45d6cdad5b7",
            "e04d99d66e564dff85f66b26c1b5a22c",
            "01b2b540048f41558cf6003da766f17d",
            "29229a9552454b349da0b05611cbae4c",
            "5aef7e7e803e48e1abd79d9c0cd575c1",
            "9ca4e9cbd8084127aa7615ee6435c50f",
            "b41928d3d65146e4b81f5539a0ccca8f",
            "a33ae500a3804e59ba1df49d406bb9e8"
          ]
        },
        "id": "P6UJEk6a2YCt",
        "outputId": "adf25b48-494b-4d3c-f203-3c632c123dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/983.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m983.0/983.2 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing device: cuda\n",
            "NUM_FINE (species classes) = 35\n",
            "NUM_COARSE (coarse groups) = 2\n",
            "Train images (Endless Forams): 22183\n",
            "Val images   (Endless Forams): 5546\n",
            "Unlabeled pool images (MD022508 + MD972138): 28275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30f7ef2f8eeb484eaaecff250a931d3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models._builder:Unexpected keys (norm.bias, norm.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Pretraining on synthetic fractal data -----\n",
            "[Pretrain] Epoch 1/2 - loss: 3.9457\n",
            "[Pretrain] Epoch 2/2 - loss: 3.8564\n",
            "\n",
            "----- Fine-tuning on Endless Forams -----\n",
            "[Finetune] Epoch 1/2 train_loss=3.8364 val_loss=3.3174 coarse_acc=0.4999 fine_acc=0.0369 fine_f1=0.0175\n",
            "  -> saved best model (F1 = 0.0175)\n",
            "[Finetune] Epoch 2/2 train_loss=3.6429 val_loss=3.2632 coarse_acc=0.5367 fine_acc=0.0428 fine_f1=0.0269\n",
            "  -> saved best model (F1 = 0.0269)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DKXxNZoi2X_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bGSshEmh2X8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eRIWYkG32X5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ibYxhI832X2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g6vJefwk2Xym"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}